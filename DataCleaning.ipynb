{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f75113f4-ddb7-4f80-9326-36668e68e8fc",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c517fff-97c6-497f-92dd-09cb88729f6d",
   "metadata": {},
   "source": [
    "#### 1. Load the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae5a4330-273c-4ad0-bbe9-eaf63e4475c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "import spacy\n",
    "import string\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d19131-0795-406e-bc59-184bb9ede64e",
   "metadata": {},
   "source": [
    "#### 2. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c57326d-a9b3-45d3-9361-ff1d221dbb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Amazon_Food_Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa3dd347-8fc6-49a0-a99a-52bb115ef434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductId</th>\n",
       "      <td>B001E4KFG0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserId</th>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProfileName</th>\n",
       "      <td>delmartian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <td>1303862400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Summary</th>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        0\n",
       "Id                                                                      1\n",
       "ProductId                                                      B001E4KFG0\n",
       "UserId                                                     A3SGXH7AUHU8GW\n",
       "ProfileName                                                    delmartian\n",
       "HelpfulnessNumerator                                                    1\n",
       "HelpfulnessDenominator                                                  1\n",
       "Score                                                                   5\n",
       "Time                                                           1303862400\n",
       "Summary                                             Good Quality Dog Food\n",
       "Text                    I have bought several of the Vitality canned d..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17caff-967a-458c-80e2-9be9d8ea4470",
   "metadata": {},
   "source": [
    "#### 3. Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21813c48-4c6f-46e5-a3e8-d76c86f209cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                         0\n",
       "ProductId                  0\n",
       "UserId                     0\n",
       "ProfileName               26\n",
       "HelpfulnessNumerator       0\n",
       "HelpfulnessDenominator     0\n",
       "Score                      0\n",
       "Time                       0\n",
       "Summary                   27\n",
       "Text                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values = df.isna().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a491b2d4-0d1a-4f68-b988-89a2f1e16275",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['ProfileName', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4825286f-ba0b-4d68-9eb7-a78563cfbcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                        0\n",
       "ProductId                 0\n",
       "UserId                    0\n",
       "ProfileName               0\n",
       "HelpfulnessNumerator      0\n",
       "HelpfulnessDenominator    0\n",
       "Score                     0\n",
       "Time                      0\n",
       "Summary                   0\n",
       "Text                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values = df.isna().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9faaceb-4c1b-4156-93c4-6b31263b6dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2011-04-27\n",
       "1        2012-09-07\n",
       "2        2008-08-18\n",
       "3        2011-06-13\n",
       "4        2012-10-21\n",
       "            ...    \n",
       "568449   2011-03-09\n",
       "568450   2012-03-09\n",
       "568451   2012-02-21\n",
       "568452   2012-03-13\n",
       "568453   2012-05-31\n",
       "Name: Time_converted, Length: 568401, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Converting Date-Time to readable format\n",
    "df['Time_converted'] = pd.to_datetime(df['Time'], unit='s')\n",
    "df['Time_converted']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5963d2e6-638f-4cc3-b792-e967594a4e39",
   "metadata": {},
   "source": [
    "#### 4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2584cf9d-ffea-4f0f-97d9-0fdf102720a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the total sample size\n",
    "total_sample_size = 5000\n",
    "\n",
    "# Count instances in each stratum\n",
    "stratum_counts = df['Score'].value_counts()\n",
    "\n",
    "# Determine sample size for each stratum\n",
    "sample_sizes = (stratum_counts / stratum_counts.sum() * total_sample_size).round().astype(int)\n",
    "\n",
    "# List to hold sampled data\n",
    "samples = []\n",
    "\n",
    "# Randomly sample from each stratum\n",
    "for score, size in sample_sizes.items():\n",
    "    stratum_samples = df[df['Score'] == score].sample(n=size, random_state=1)\n",
    "    samples.append(stratum_samples)\n",
    "\n",
    "# Combine samples into a final DataFrame\n",
    "final_sample = pd.concat(samples)\n",
    "\n",
    "# print(final_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f928aec1-ab85-4803-b03e-24b5b224d130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score\n",
       "5    3194\n",
       "4     709\n",
       "1     460\n",
       "3     375\n",
       "2     262\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count instances in each stratum\n",
    "stratum_counts = final_sample['Score'].value_counts()\n",
    "stratum_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adaa26a6-e291-438a-8b1b-da9d3a9bbff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     Text  \\\n",
      "519868  I love these chips! This is the best snack eve...   \n",
      "518974  This is a really good chocolate candy, has jus...   \n",
      "55123   My wife has a male cat who throws up EVERYTHIN...   \n",
      "508510  This entire product line is absolutely great! ...   \n",
      "155616  I use this to keep my blood level stable all d...   \n",
      "...                                                   ...   \n",
      "226450  As with Mr. Carlson's review, my order was als...   \n",
      "437066  I've been ordering a lot of these. Many of the...   \n",
      "366627  The Dogswell Happy Hips Sweet Potato Chews I b...   \n",
      "230944  Just so you understand me, my last meal on ear...   \n",
      "126886  These traps work and they require less excavat...   \n",
      "\n",
      "                                                   lemmas  \n",
      "519868  I love these chip ! this be the good snack eve...  \n",
      "518974  this be a really good chocolate candy , have j...  \n",
      "55123   my wife have a male cat who throw up everythin...  \n",
      "508510  this entire product line be absolutely great !...  \n",
      "155616  I use this to keep my blood level stable all d...  \n",
      "...                                                   ...  \n",
      "226450  as with Mr. Carlson 's review , my order be al...  \n",
      "437066  I have be order a lot of these . many of these...  \n",
      "366627  the Dogswell Happy Hips Sweet Potato Chews I b...  \n",
      "230944  just so you understand I , my last meal on ear...  \n",
      "126886  these trap work and they require less excavati...  \n",
      "\n",
      "[5000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Lemmatization function for batch processing\n",
    "def lemmatize_batch(texts):\n",
    "    docs = nlp.pipe(texts, disable=[\"ner\", \"parser\"], batch_size=100)\n",
    "    return [\" \".join([token.lemma_ for token in doc]) for doc in docs]\n",
    "\n",
    "# Apply the lemmatization function to the DataFrame in batches\n",
    "final_sample['lemmas'] = lemmatize_batch(final_sample['Text'].tolist())\n",
    "\n",
    "# Display the DataFrame with lemmas\n",
    "print(final_sample[['Text', 'lemmas']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08527f41-849c-41ae-a5d0-d52db4ed76e9",
   "metadata": {},
   "source": [
    "#### 5. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "394b79aa-684d-439e-8cf4-047982c14e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     Text  \\\n",
      "519868  I love these chips! This is the best snack eve...   \n",
      "518974  This is a really good chocolate candy, has jus...   \n",
      "55123   My wife has a male cat who throws up EVERYTHIN...   \n",
      "508510  This entire product line is absolutely great! ...   \n",
      "155616  I use this to keep my blood level stable all d...   \n",
      "...                                                   ...   \n",
      "226450  As with Mr. Carlson's review, my order was als...   \n",
      "437066  I've been ordering a lot of these. Many of the...   \n",
      "366627  The Dogswell Happy Hips Sweet Potato Chews I b...   \n",
      "230944  Just so you understand me, my last meal on ear...   \n",
      "126886  These traps work and they require less excavat...   \n",
      "\n",
      "                                                   lemmas  \\\n",
      "519868  I love these chip ! this be the good snack eve...   \n",
      "518974  this be a really good chocolate candy , have j...   \n",
      "55123   my wife have a male cat who throw up everythin...   \n",
      "508510  this entire product line be absolutely great !...   \n",
      "155616  I use this to keep my blood level stable all d...   \n",
      "...                                                   ...   \n",
      "226450  as with Mr. Carlson 's review , my order be al...   \n",
      "437066  I have be order a lot of these . many of these...   \n",
      "366627  the Dogswell Happy Hips Sweet Potato Chews I b...   \n",
      "230944  just so you understand I , my last meal on ear...   \n",
      "126886  these trap work and they require less excavati...   \n",
      "\n",
      "                                                   tokens  \n",
      "519868          [love, chip, good, snack, healthy, combo]  \n",
      "518974  [good, chocolate, candy, right, mixture, raspb...  \n",
      "55123   [wife, male, cat, throw, food, cat, prefer, fo...  \n",
      "508510  [entire, product, line, absolutely, great, fit...  \n",
      "155616  [use, blood, level, stable, day, work, great, ...  \n",
      "...                                                   ...  \n",
      "226450  [mr, carlson, s, review, order, receive, damag...  \n",
      "437066  [order, lot, directly, daughter, s, house, let...  \n",
      "366627  [dogswell, happy, hips, sweet, potato, chews, ...  \n",
      "230944  [understand, meal, earth, sour, gummy, worm, d...  \n",
      "126886  [trap, work, require, excavation, knowledge, u...  \n",
      "\n",
      "[5000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function with punctuation and stop word removal and lowercasing\n",
    "def tokenize_batch(texts):\n",
    "    docs = nlp.pipe(texts, disable=[\"ner\", \"parser\"], batch_size=100)\n",
    "    return [\n",
    "        [token.text.lower() for token in doc \n",
    "         if token.text not in string.punctuation \n",
    "         and not token.is_stop \n",
    "         and token.text.strip()]  # Remove empty strings\n",
    "        for doc in docs\n",
    "    ]\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove all characters except letters\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)  # Allow spaces for tokenization\n",
    "\n",
    "# Apply the lemmatization function\n",
    "final_sample['lemmas'] = lemmatize_batch(final_sample['Text'].tolist())\n",
    "\n",
    "# Clean the lemmas to keep only letters\n",
    "final_sample['cleaned_lemmas'] = final_sample['lemmas'].apply(clean_text)\n",
    "\n",
    "# Apply the tokenization function to the cleaned lemmas\n",
    "final_sample['tokens'] = tokenize_batch(final_sample['cleaned_lemmas'].tolist())\n",
    "\n",
    "# Display the DataFrame with original text, lemmas, and tokens\n",
    "print(final_sample[['Text', 'lemmas', 'tokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3fa6cdc-a269-439e-b7e0-969261caafd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # Simple tokenization\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "final_sample['tokens_simple'] = final_sample['Text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a5e0ba7-b6c6-497b-917c-35775d36f203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>519868</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <td>519869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductId</th>\n",
       "      <td>B000YSTIL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserId</th>\n",
       "      <td>A3R8GMMQBX0OAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProfileName</th>\n",
       "      <td>Ma Princesse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <td>1272067200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Summary</th>\n",
       "      <td>Best Snack Ever!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text</th>\n",
       "      <td>I love these chips! This is the best snack eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time_converted</th>\n",
       "      <td>2010-04-24 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmas</th>\n",
       "      <td>I love these chip ! this be the good snack eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleaned_lemmas</th>\n",
       "      <td>I love these chip  this be the good snack ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>[love, chip, good, snack, healthy, combo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_simple</th>\n",
       "      <td>[i, love, these, chips, this, is, the, best, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   519868\n",
       "Id                                                                 519869\n",
       "ProductId                                                      B000YSTIL0\n",
       "UserId                                                     A3R8GMMQBX0OAD\n",
       "ProfileName                                                  Ma Princesse\n",
       "HelpfulnessNumerator                                                    0\n",
       "HelpfulnessDenominator                                                  0\n",
       "Score                                                                   5\n",
       "Time                                                           1272067200\n",
       "Summary                                                 Best Snack Ever!!\n",
       "Text                    I love these chips! This is the best snack eve...\n",
       "Time_converted                                        2010-04-24 00:00:00\n",
       "lemmas                  I love these chip ! this be the good snack eve...\n",
       "cleaned_lemmas          I love these chip  this be the good snack ever...\n",
       "tokens                          [love, chip, good, snack, healthy, combo]\n",
       "tokens_simple           [i, love, these, chips, this, is, the, best, s..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sample.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "954a3853-f083-48e9-9c79-bdafca2ae32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save as csv file\n",
    "final_sample.to_csv('Amazon_Food_Reviews_cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
